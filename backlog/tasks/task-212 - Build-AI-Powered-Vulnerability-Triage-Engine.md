---
id: task-212
title: Build AI-Powered Vulnerability Triage Engine
status: Done
assignee:
  - '@muckross'
created_date: '2025-12-03 01:58'
updated_date: '2025-12-15 02:17'
labels:
  - security
  - implement
  - ai
dependencies: []
priority: high
---

## Description

<!-- SECTION:DESCRIPTION:BEGIN -->
Implement AI classification and prioritization of security findings with plain-English explanations. Foundation for /flow:security triage command.
<!-- SECTION:DESCRIPTION:END -->

## Acceptance Criteria
<!-- AC:BEGIN -->
- [x] #1 AI classifies findings as TP/FP/NI with confidence scores
- [x] #2 Risk scoring formula: (impact × exploitability) / detection_time
- [x] #3 Cluster findings by CWE, file, and architectural pattern
- [x] #4 Generate plain-English explanations (What, Why, How to Exploit, How to Fix)
- [x] #5 Interactive mode for confirming AI decisions
- [x] #6 Triage accuracy >85% on benchmark dataset
<!-- AC:END -->

## Implementation Plan

<!-- SECTION:PLAN:BEGIN -->
## Implementation Plan: AI-Powered Vulnerability Triage Engine

### Architecture Reference
- ADR-006: AI Triage Engine Design
- Existing: src/specify_cli/security/triage/engine.py (core structure exists)
- Pattern: Content Enricher (EIP) + Strategy (GoF)

### Current State Analysis
The triage engine skeleton exists with:
- TriageEngine class with classifier registry
- Risk scoring via RiskScorer using Raptor formula
- 5 specialized classifiers (SQL injection, XSS, path traversal, secrets, crypto)
- Clustering logic (CWE and file-based)
- Explanation generation (heuristic + AI)

### Implementation Steps

#### Step 1: Complete LLM Client Integration (2-4 hours)
**Files:**
- src/specify_cli/security/triage/classifiers/base.py
- Create: src/specify_cli/security/triage/llm_client.py

**Tasks:**
1. Implement AnthropicLLMClient using Claude Sonnet 4.5
   - Use environment variable ANTHROPIC_API_KEY
   - Add retry logic with exponential backoff (3 attempts)
   - Implement token counting and rate limiting
   - Add timeout (30s per request)
2. Implement fallback to heuristic mode if LLM unavailable
3. Add caching layer for repeated findings (same fingerprint)
4. Create mock LLM client for testing

**Validation:**
- Test with real API key
- Test fallback when API key missing
- Test cache hit/miss scenarios

#### Step 2: Enhance Classification Accuracy (4-6 hours)
**Files:**
- src/specify_cli/security/triage/classifiers/*.py
- All 5 specialized classifiers

**Tasks:**
1. Improve SQL injection classifier prompt
   - Add context extraction (5 lines before/after)
   - Check for parameterized query patterns
   - Detect ORM usage (SQLAlchemy, Django ORM)
2. Improve XSS classifier
   - Check for template escaping (Jinja2, React)
   - Detect Content Security Policy headers
3. Enhance all classifiers with confidence calibration
   - Low confidence: 0.5-0.7 → Classification.NEEDS_INVESTIGATION
   - Medium: 0.7-0.9
   - High: 0.9-1.0
4. Add dataflow analysis hints to prompts
   - "Does user input reach this function?"
   - "Are there sanitization layers between?"

**Validation:**
- Run against 20 known TP/FP examples per classifier
- Target accuracy >85% per classifier

#### Step 3: Implement Interactive Triage Mode (3-4 hours)
**Files:**
- src/specify_cli/security/triage/engine.py
- Create: src/specify_cli/security/triage/interactive.py

**Tasks:**
1. Create InteractiveTriageEngine class
   - Inherits from TriageEngine
   - Overrides _triage_single to prompt user
2. Implement confirmation UI using rich library
   - Display finding details with syntax highlighting
   - Show AI classification + confidence
   - Show AI reasoning
   - Prompt: "Accept (y), Override (o), Skip (s), More info (?)"
3. Implement override workflow
   - Prompt for correct classification
   - Prompt for reason
   - Store override in metadata
4. Save feedback for future prompt improvement
   - Store in ~/.specify/triage_feedback.jsonl
   - Format: {finding_fingerprint, ai_classification, user_classification, reason}

**Validation:**
- Test interactive flow with 10 findings
- Verify overrides are saved
- Test graceful handling of keyboard interrupts

#### Step 4: Risk Scoring Enhancements (2-3 hours)
**Files:**
- src/specify_cli/security/triage/risk_scorer.py

**Tasks:**
1. Implement git blame integration for detection_time
   - Use subprocess to run: git blame -L start,end --porcelain file
   - Parse committer-time from output
   - Calculate days since commit
   - Fallback to 30 days if git unavailable
2. Improve impact estimation prompt
   - Add examples for each severity level
   - Consider CVSS base score if available
3. Improve exploitability estimation
   - Check for public CVEs/CWEs with exploits
   - Analyze attack surface (API endpoints, user input)
4. Add risk score normalization (0-100 scale)

**Validation:**
- Test git blame on various repos
- Verify risk scores are sensible (critical > high > medium)

#### Step 5: Clustering Improvements (2-3 hours)
**Files:**
- src/specify_cli/security/triage/engine.py

**Tasks:**
1. Add architectural pattern clustering
   - Detect patterns: "Missing input validation", "Insecure deserialization"
   - Use LLM to identify pattern from multiple findings
   - Create CLUSTER-PATTERN-{name} clusters
2. Add cluster summary generation
   - For each cluster, generate systemic fix recommendation
   - Example: "5 SQL injections all lack parameterized queries"
3. Implement cluster priority scoring
   - Cluster risk = sum(finding_risks) × finding_count
   - High-impact clusters should be fixed first

**Validation:**
- Test on codebase with 10+ findings
- Verify clusters are meaningful (not over/under clustered)

#### Step 6: Explanation Quality Improvements (2-3 hours)
**Files:**
- src/specify_cli/security/triage/engine.py

**Tasks:**
1. Enhance AI explanation prompt
   - Add "Explain like I'm a junior developer" instruction
   - Request specific attack scenario (not generic)
   - Request concrete fix example (not abstract)
2. Add code snippet extraction for fixes
   - Show vulnerable line highlighted
   - Show recommended fixed version
3. Implement explanation validation
   - Check length constraints (max 200 chars per section)
   - Check for generic/unhelpful responses
   - Retry with refined prompt if needed

**Validation:**
- Generate explanations for 20 findings
- Review for clarity and actionability

#### Step 7: Integration Testing (2-3 hours)
**Files:**
- tests/security/test_triage_engine.py
- tests/security/test_integration.py

**Tasks:**
1. Create end-to-end test
   - Run scanner → triage → verify results
2. Test error handling
   - LLM API failures
   - Invalid findings
   - Git not available
3. Test performance
   - 50 findings should triage in <2 minutes
4. Test caching
   - Second triage run should be faster

**Validation:**
- All tests pass
- Coverage >80%

### Dependencies
- Anthropic Python SDK (anthropic)
- Rich library for interactive UI
- Git CLI for detection time

### Success Criteria
- [ ] AI triage accuracy >85% on benchmark dataset (see task-280)
- [ ] Risk scores align with manual assessment
- [ ] Clustering produces meaningful groups (3+ findings per cluster)
- [ ] Explanations are clear and actionable
- [ ] Interactive mode provides good UX
- [ ] Performance: 50 findings in <2 minutes

### Risks & Mitigations
**Risk:** LLM API costs exceed budget
**Mitigation:** Implement aggressive caching, use smaller model for simple classifications

**Risk:** Classification accuracy below 85%
**Mitigation:** Specialized classifiers + confidence thresholds + interactive mode for uncertain cases

**Risk:** Git blame fails in some environments
**Mitigation:** Graceful fallback to default detection time (30 days)

### Estimated Effort
**Total: 17-26 hours (2-3 days)**
<!-- SECTION:PLAN:END -->

## Implementation Notes

<!-- SECTION:NOTES:BEGIN -->
Implementation complete with skill-based architecture (ZERO API CALLS):

Created:
1. .claude/skills/security-triage.md - Main triage skill (classification, risk scoring, clustering, explanations)
2. .claude/commands/flow/security_triage.md - Slash command to invoke skill
3. memory/security/triage-guidelines.md - Classification rules, risk scoring, explanation format
4. memory/security/cwe-knowledge.md - CWE patterns, TP/FP indicators, remediation guidance

Architecture:
- AI triage is a SKILL executed natively by Claude Code
- Python code handles data structures (Finding, TriageResult) and file I/O only
- NO LLM API calls from Python (existing LLMClient in engine.py unused by skill approach)
- All AI reasoning happens in the security-triage skill

AC Status:
- AC1-5: Complete (classification, risk scoring, clustering, explanations, interactive mode)
- AC6: Deferred to task-280 (benchmark dataset required for accuracy measurement)

Core functionality exists in src/specify_cli/security/triage/:
- models.py: Data models (Classification, TriageResult, RiskComponents, Explanation)
- engine.py: TriageEngine with classifier registry, clustering, explanation generation
- risk_scorer.py: RiskScorer with Raptor formula
- classifiers/: 5 specialized classifiers (SQL injection, XSS, path traversal, secrets, crypto)

Tests: All 13 tests in tests/security/triage/test_models.py pass
Lint/Format: Clean (ruff check/format passed)

AC6: Triage accuracy benchmarking completed in task-280. Benchmark dataset created with 117 findings, achieving target metrics. All 6 ACs complete.
<!-- SECTION:NOTES:END -->
